# -*- coding: utf-8 -*-
"""dashboard-meta-analysis1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P3vX7_i-WwarnU0wLwZeBzUaOlLZ1-_W

# **Identifying Themes from "Men's Rights" Subreddit Feeds**: Dashboard Visualizations

This notebook provides sample code of visualizations to be used for a dashboard that aims to provide an overview and meta-analysis of text data scraped from Men's Rights Activist Subreddit feeds, which include the quarantined `r/TheRedPill` and `r/MGOTW`, as well as `r/MensRights`. We envision this project to be an interactive dashboard of (1) time-series plots for up-votes and number of comments and (2) interactive results from topic models of titles and body text within each feed. 

The text data from Reddit was scraped using the Python Reddit API Wrapper (PRAW), which allowed us to extract 100 posts from the following categories: (1) top (2) hot (3) new (4) controversial (5) rising. Data from each category within each subreddit is stored as their own data frame, which can be selected by the user based on which feed they are interested in viewing. The visualizations in this notebook reflect some of the output code from the options we aim to implement in the final version of this project.

# Setup code
"""

import pandas as pd
import numpy as np 
import os

import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
from google.colab import files

# Uncomment the line below and rerun if you generate an error 
#!pip install chart-studio
import chart_studio
import chart_studio.plotly as py
import plotly.graph_objects as go

drive.mount('/content/drive')

"""# Code for the Python Reddit API Wrapper (`praw`)"""

#!pip install praw
import praw

#TOP 
top_subreddit = subreddit.top()

for submission in subreddit.top(limit=1):
    print(submission.title, submission.id)

topics_dict = { "title":[], 
                "ups":[],
                "downs":[],
                "score":[], 
                "id":[], 
                "url":[], 
                "comms_num": [], 
                "created": [], 
                "body": [], 
                "date": []}

for submission in top_subreddit:
    topics_dict["title"].append(submission.title)
    topics_dict["ups"].append(submission.ups)
    topics_dict["downs"].append(submission.downs)
    topics_dict["score"].append(submission.score)
    topics_dict["id"].append(submission.id)
    topics_dict["url"].append(submission.url)
    topics_dict["comms_num"].append(submission.num_comments)
    topics_dict["created"].append(submission.created)
    topics_dict["body"].append(submission.selftext)
    topics_dict["date"].append(get_date(submission))

top_df = pd.DataFrame(topics_dict)
top_df

#HOT

hot_subreddit = subreddit.hot()

for submission in subreddit.hot(limit=1):
    print(submission.title, submission.id)

topics_dict = { "title":[], 
                "ups":[],
                "downs":[],
                "score":[], 
                "id":[], 
                "url":[], 
                "comms_num": [], 
                "created": [], 
                "body": [], 
                "date": []}

for submission in hot_subreddit:
    topics_dict["title"].append(submission.title)
    topics_dict["ups"].append(submission.ups)
    topics_dict["downs"].append(submission.downs)
    topics_dict["score"].append(submission.score)
    topics_dict["id"].append(submission.id)
    topics_dict["url"].append(submission.url)
    topics_dict["comms_num"].append(submission.num_comments)
    topics_dict["created"].append(submission.created)
    topics_dict["body"].append(submission.selftext)
    topics_dict["date"].append(get_date(submission))

hot_df = pd.DataFrame(topics_dict)
hot_df

# NEW 

new_subreddit = subreddit.new()

for submission in subreddit.new(limit=1):
    print(submission.title, submission.id)

topics_dict = { "title":[], 
                "ups":[],
                "downs":[],
                "score":[], 
                "id":[], 
                "url":[], 
                "comms_num": [], 
                "created": [], 
                "body": [], 
                "date": []}

for submission in new_subreddit:
    topics_dict["title"].append(submission.title)
    topics_dict["ups"].append(submission.ups)
    topics_dict["downs"].append(submission.downs)
    topics_dict["score"].append(submission.score)
    topics_dict["id"].append(submission.id)
    topics_dict["url"].append(submission.url)
    topics_dict["comms_num"].append(submission.num_comments)
    topics_dict["created"].append(submission.created)
    topics_dict["body"].append(submission.selftext)
    topics_dict["date"].append(get_date(submission))

new_df = pd.DataFrame(topics_dict)
new_df

"""# Loading Data from Subreddits"""

data_path = "/content/drive/My Drive/___hertie/2020-spring (1)/rsub-redpill-analysis/data" # CHANGE THIS TO REFLECT WHERE THE DATA FOLDER IS IN YOUR DRIVE 

con_df1 = pd.read_csv(os.path.join(data_path,"01_controversial.csv"))
hot_df1 = pd.read_csv(os.path.join(data_path,"01_hot.csv"))
new_df1 = pd.read_csv(os.path.join(data_path,"01_new.csv"))
rising_df1 = pd.read_csv(os.path.join(data_path,"01_rising.csv"))
top_df1 = pd.read_csv(os.path.join(data_path,"01_top.csv"))

con_df2 = pd.read_csv(os.path.join(data_path,"02_controversial.csv"))
hot_df2 = pd.read_csv(os.path.join(data_path,"02_hot.csv"))
new_df2 = pd.read_csv(os.path.join(data_path,"02_new.csv"))
rising_df2 = pd.read_csv(os.path.join(data_path,"02_rising.csv"))
top_df2 = pd.read_csv(os.path.join(data_path,"02_top.csv"))

con_df3 = pd.read_csv(os.path.join(data_path,"03_controversial.csv"))
hot_df3 = pd.read_csv(os.path.join(data_path,"03_hot.csv"))
new_df3 = pd.read_csv(os.path.join(data_path,"03_new.csv"))
rising_df3 = pd.read_csv(os.path.join(data_path,"03_rising.csv"))
top_df3 = pd.read_csv(os.path.join(data_path,"03_top.csv"))

# add year variable
con_df1['year'] = con_df1['date'].astype(str).str[:4]
hot_df1['year'] = hot_df1['date'].astype(str).str[:4]
new_df1['year'] = new_df1['date'].astype(str).str[:4]
rising_df1['year'] = rising_df1['date'].astype(str).str[:4]
top_df1['year'] = top_df1['date'].astype(str).str[:4]

con_df2['year'] = con_df2['date'].astype(str).str[:4]
hot_df2['year'] = hot_df2['date'].astype(str).str[:4]
new_df2['year'] = new_df2['date'].astype(str).str[:4]
rising_df2['year'] = rising_df2['date'].astype(str).str[:4]
top_df2['year'] = top_df2['date'].astype(str).str[:4]

con_df3['year'] = con_df3['date'].astype(str).str[:4]
hot_df3['year'] = hot_df3['date'].astype(str).str[:4]
new_df3['year'] = new_df3['date'].astype(str).str[:4]
rising_df3['year'] = rising_df3['date'].astype(str).str[:4]
top_df3['year'] = top_df3['date'].astype(str).str[:4]

# change variable type for dates and reorder 
top_df1['date'] = pd.to_datetime(top_df1['date'])
top_df1 = top_df1.sort_values(by='date')

top_df2['date'] = pd.to_datetime(top_df2['date'])
top_df2 = top_df2.sort_values(by='date')

top_df3['date'] = pd.to_datetime(top_df3['date'])
top_df3 = top_df3.sort_values(by='date')

con_df1['date'] = pd.to_datetime(con_df1['date'])
con_df1 = con_df1.sort_values(by='date')

con_df2['date'] = pd.to_datetime(con_df2['date'])
con_df2 = con_df2.sort_values(by='date')

con_df3['date'] = pd.to_datetime(con_df3['date'])
con_df3 = con_df3.sort_values(by='date')

# set indices for plotting ts graphs 
top_df1 = top_df1.set_index(['date']) 
top_df2 = top_df2.set_index(['date']) 
top_df3 = top_df3.set_index(['date'])

con_df1 = con_df1.set_index(['date']) 
con_df2 = con_df2.set_index(['date']) 
con_df3 = con_df3.set_index(['date'])

# Initialization step for authenticating plotly account with chart-studio 
# To get API key, go to: https://plot.ly/settings/api
chart_studio.tools.set_credentials_file(username='allisonkoh',                                              
                                  api_key='3sIkFG51qFTXbqi8Zm8u')

"""# Visualization

## Scores and Number of Comments over Time

We can learn a lot from looking at the scores (difference betwen upvotes and downvotes) and number of post comments per month. These interactive charts that are a crucial part of exploratory data analysis then lead us to identify spikes in activity and observe activity around certain key dates related to the Incel movement. 

To guide possible questions, we plan on offering options to plot multiple lines in one graph (one per Subreddit/category) and include the option of highlighting key events relevant to Incel activity.
"""

score_series = top_df1.loc[:,('comms_num')]
score_series.info

# Write series code 
score_series = top_df1.loc[:,('comms_num')]
# score_series.plot(figsize=(10,8));

# Set x and y 
score_series = go.Scatter(x=score_series.index,
                         y=score_series.values)

# Set layout 
layout = go.Layout(title='`r/TheRedPill`: Number of comments among top 100 posts', xaxis=dict(title='Date'),
                   yaxis=dict(title='# Comments'))

# Plot
fig = go.Figure(data=[score_series], layout=layout)
py.iplot(fig, sharing='public')

# Write series code 
score_series = top_df1.loc[:,('score')]
# score_series.plot(figsize=(10,8));

# Set x and y 
score_series  = go.Scatter(x=score_series.index,
                         y=score_series.values)

# Set layout 
layout = go.Layout(title='`r/TheRedPill`: Score Among top 100 posts', xaxis=dict(title='Date'),
                   yaxis=dict(title='Score'))

# Plot
fig = go.Figure(data=[score_series], layout=layout)
py.iplot(fig, sharing='public')

# Write series code 
score_series = con_df1.loc[:,('comms_num')]
# score_series.plot(figsize=(10,8));

# Set x and y 
score_series = go.Scatter(x=score_series.index,
                         y=score_series.values)

# Set layout 
layout = go.Layout(title='`r/TheRedPill`: Number of Comments among 100 most controversial posts', xaxis=dict(title='Date'),
                   yaxis=dict(title='# Comments'))

# Plot
fig = go.Figure(data=[score_series], layout=layout)
py.iplot(fig, sharing='public')

# Write series code 
score_series = con_df1.loc[:,('score')]
#score_series.plot(figsize=(10,8));

# Set x and y 
score_series  = go.Scatter(x=score_series.index,
                         y=score_series.values)

# Set layout 
layout = go.Layout(title='`r/TheRedPill`: Score among 100 most controversial posts', xaxis=dict(title='Date'),
                   yaxis=dict(title='Score'))

# Plot
fig = go.Figure(data=[score_series], layout=layout)
py.iplot(fig, sharing='public')

# Write series code 
score_series = top_df2.loc[:,("comms_num")]
#score_series.plot(figsize=(10,8));

# Set x and y 
score_series  = go.Scatter(x=score_series.index,
                         y=score_series.values)

# Set layout 
layout = go.Layout(title='`r/MGTOW`: Number of comments among top 100 posts', xaxis=dict(title='Date'),
                   yaxis=dict(title='# Comments'))

# Plot
fig = go.Figure(data=[score_series], layout=layout)
py.iplot(fig, sharing='public')

# Write series code 
score_series = top_df2.loc[:,("ups")]
#score_series.plot(figsize=(10,8));

# Set x and y 
score_series  = go.Scatter(x=score_series.index,
                         y=score_series.values)

# Set layout 
layout = go.Layout(title='`r/MGTOW`: Number of upvotes among top 100 posts', xaxis=dict(title='Date'),
                   yaxis=dict(title='(no. upvotes)'))

# Plot
fig = go.Figure(data=[score_series], layout=layout)
py.iplot(fig, sharing='public')

# Write series code 
score_series = con_df2.loc[:,("comms_num")]
#score_series.plot(figsize=(10,8));

# Set x and y 
score_series  = go.Scatter(x=score_series.index,
                         y=score_series.values)

# Set layout 
layout = go.Layout(title='`r/MGTOW`: Number of comments among most controversial 100 posts', xaxis=dict(title='Date'),
                   yaxis=dict(title='(no. Comments )'))

# Plot
fig = go.Figure(data=[score_series], layout=layout)
py.iplot(fig, sharing='public')

# Write series code 
score_series = con_df2.loc[:,("score")]
#score_series.plot(figsize=(10,8));

# Set x and y 
score_series  = go.Scatter(x=score_series.index,
                         y=score_series.values)

# Set layout 
layout = go.Layout(title='`r/MGTOW`: Score among most controversial 100 posts', xaxis=dict(title='Date'),
                   yaxis=dict(title='(score)'))

# Plot
fig = go.Figure(data=[score_series], layout=layout)
py.iplot(fig, sharing='public')

# Write series code 
score_series = top_df3.loc[:,("comms_num")]
#score_series.plot(figsize=(10,8));

# Set x and y 
score_series  = go.Scatter(x=score_series.index,
                         y=score_series.values)

# Set layout 
layout = go.Layout(title='`r/MensRights`: Number of Comments among top 100 posts', xaxis=dict(title='Date'),
                   yaxis=dict(title='# Comments'))

# Plot
fig = go.Figure(data=[score_series], layout=layout)
py.iplot(fig, sharing='public')

# Write series code 
score_series = top_df3.loc[:,("score")]
#score_series.plot(figsize=(10,8));

# Set x and y 
score_series  = go.Scatter(x=score_series.index,
                         y=score_series.values)

# Set layout 
layout = go.Layout(title='`r/MensRights`: Score among top 100 posts', xaxis=dict(title='Date'),
                   yaxis=dict(title='(score)'))

# Plot
fig = go.Figure(data=[score_series], layout=layout)
py.iplot(fig, sharing='public')

# Write series code 
score_series = con_df3.loc[:,("comms_num")]
#score_series.plot(figsize=(10,8));

# Set x and y 
score_series  = go.Scatter(x=score_series.index,
                         y=score_series.values)

# Set layout 
layout = go.Layout(title='`r/MensRights`: Number of comments among 100 most controversial posts', xaxis=dict(title='Date'),
                   yaxis=dict(title='(no. Comments )'))

# Plot
fig = go.Figure(data=[score_series], layout=layout)
py.iplot(fig, sharing='public')

# Write series code 
score_series = con_df3.loc[:,("score")]
#score_series.plot(figsize=(10,8));

# Set x and y 
score_series  = go.Scatter(x=score_series.index,
                         y=score_series.values)

# Set layout 
layout = go.Layout(title='`r/MensRights`: Score among 100 most controversial posts', xaxis=dict(title='Date'),
                   yaxis=dict(title='(score )'))

# Plot
fig = go.Figure(data=[score_series], layout=layout)
py.iplot(fig, sharing='public')

"""# Basic Topic Models

At this stage of the project, interactive visualizations of basic topic models remains a work in progress. As offering unstructured text in its original form would violate best practices with regards to ethics, topic models offer insight on trends within the text data from the title and body of extracted posts.
"""

# import data 
data_path = "/content/drive/My Drive/___hertie/2020-spring (1)/rsub-redpill-analysis/data"
df = pd.read_csv(os.path.join(data_path,"Text analysis  - Sheet1.csv"))

# load regex library 
import re 

# drop duplicate posts 
df1 = df.drop_duplicates(['id'])

# rm NA
df2t = df1.dropna(subset=['title'])
df2t = df2t.loc[:, ['id', 'title']]

df2b = df1.dropna(subset=['body'])
df2b = df2b.loc[:, ['id', 'body']]
df2b['title'] = df2b['body']
df2b = df2b.loc[:, ['id', 'title']]

# combine dfs with selected rows 
df2 = df2t.append(df2b)

# preprocess text 
df2['text_processed'] = df2['title'].map(lambda x: re.sub('[,\.!?]', '', x)).map(lambda x: x.lower())

from wordcloud import WordCloud

# Join the different processed titles together.
long_string = ','.join(list(con_df1['title_processed'].values))

# exploratory analysis 
from wordcloud import WordCloud

# Join the different processed titles together.
long_string = ','.join(list(df2['text_processed'].values))

# Create a WordCloud object
wordcloud = WordCloud(background_color="white", max_words=1000, contour_width=3, contour_color='steelblue')

# Generate a word cloud
wordcloud.generate(long_string)

# Visualize the word cloud
wordcloud.to_image()

# Commented out IPython magic to ensure Python compatibility.
# prepare text for lda 
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('whitegrid')
# %matplotlib inline

# Helper function
def plot_10_most_common_words(count_data, count_vectorizer):
    import matplotlib.pyplot as plt
    words = count_vectorizer.get_feature_names()
    total_counts = np.zeros(len(words))
    for t in count_data:
        total_counts+=t.toarray()[0]
    
    count_dict = (zip(words, total_counts))
    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]
    words = [w[0] for w in count_dict]
    counts = [w[1] for w in count_dict]
    x_pos = np.arange(len(words)) 
    
    plt.figure(2, figsize=(15, 15/1.6180))
    plt.subplot(title='10 most common words')
    sns.set_context("notebook", font_scale=1.25, rc={"lines.linewidth": 2.5})
    sns.barplot(x_pos, counts, palette='husl')
    plt.xticks(x_pos, words, rotation=90) 
    plt.xlabel('words')
    plt.ylabel('counts')
    plt.show()

# Initialise the count vectorizer with the English stop words
count_vectorizer = CountVectorizer(stop_words='english')

# Fit and transform the processed titles
count_data = count_vectorizer.fit_transform(df2['text_processed'])

# Visualise the 10 most common words
plot_10_most_common_words(count_data, count_vectorizer)

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import CountVectorizer

imUsingColab = False

if imUsingColab:
    !pip install gensim
    !pip install pyLDAvis
    !pip install vega
    !pip install altair

#!pip install pyLDAvis
from gensim import corpora
from gensim.models.ldamodel import LdaModel

import gensim, spacy, logging, warnings
import gensim.corpora as corpora
from gensim.utils import lemmatize, simple_preprocess
from gensim.models import CoherenceModel
import matplotlib.pyplot as plt


import pyLDAvis.gensim

!pip install nltk

import nltk

nltk.download('punkt')
nltk.download('stopwords')

from nltk.corpus import stopwords

from pprint import pprint

# tokenize 
def identify_tokens(row):
    text = row['text_processed']
    tokens = nltk.word_tokenize(text)
    token_words = [w for w in tokens if w.isalpha()]
    return token_words

data_words = df2.apply(identify_tokens, axis=1)
#data = df2.text_processed.values.tolist()
#data_words = list(sent_to_words(data))

stop_words = stopwords.words('english')
stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come', 'guy', 'really', 'find', 'tell', 'much', 'http', 'call', 'let', 'mgtow', 'day',  'always', 'way', 'never'
])

# build bigram and trigram models 
bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.
trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  
bigram_mod = gensim.models.phrases.Phraser(bigram)
trigram_mod = gensim.models.phrases.Phraser(trigram)

def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
    """Remove Stopwords, Form Bigrams, Trigrams and Lemmatization"""
    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]
    texts = [bigram_mod[doc] for doc in texts]
    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]
    texts_out = []
    nlp = spacy.load('en', disable=['parser', 'ner'])
    for sent in texts:
        doc = nlp(" ".join(sent)) 
        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
    # remove stopwords once more after lemmatization
    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    
    return texts_out

data_ready = process_words(data_words)  # processed Text Data!

# Create Dictionary
id2word = corpora.Dictionary(data_ready)

# Create Corpus: Term Document Frequency
corpus = [id2word.doc2bow(text) for text in data_ready]

# Build LDA model
lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=5, 
                                           random_state=100,
                                           update_every=1,
                                           chunksize=10,
                                           passes=10,
                                           alpha='symmetric',
                                           iterations=100,
                                           per_word_topics=True)

pprint(lda_model.print_topics())

# dominant topic and % contribution in each document 
def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):
    # Init output
    sent_topics_df = pd.DataFrame()

    # Get main topic in each document
    for i, row_list in enumerate(ldamodel[corpus]):
        row = row_list[0] if ldamodel.per_word_topics else row_list            
        # print(row)
        row = sorted(row, key=lambda x: (x[1]), reverse=True)
        # Get the Dominant topic, Perc Contribution and Keywords for each document
        for j, (topic_num, prop_topic) in enumerate(row):
            if j == 0:  # => dominant topic
                wp = ldamodel.show_topic(topic_num)
                topic_keywords = ", ".join([word for word, prop in wp])
                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)
            else:
                break
    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']

    # Add original text to the end of the output
    contents = pd.Series(texts)
    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)
    return(sent_topics_df)


df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready)

# Format
df_dominant_topic = df_topic_sents_keywords.reset_index()
df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']
df_dominant_topic.head(10)

# Display setting to show more characters in column
pd.options.display.max_colwidth = 100

sent_topics_sorteddf_mallet = pd.DataFrame()
sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')

for i, grp in sent_topics_outdf_grpd:
    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, 
                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], 
                                            axis=0)

# Reset Index    
sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)

# Format
sent_topics_sorteddf_mallet.columns = ['Topic_Num', "Topic_Perc_Contrib", "Keywords", "Representative Text"]

# Show
sent_topics_sorteddf_mallet.head(10)

df2['Document_No'] = range(0,0+len(df2))

df3 = pd.merge(df2,df_dominant_topic,how='outer',on='Document_No')
df4 = df3.drop(columns=['text_processed','text1','Text'])
df4.to_csv("topics5.csv")
files.download("topics5.csv")

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.simplefilter(action='ignore', category=DeprecationWarning)
pd.options.mode.chained_assignment = None 

random_state = 20181126

cluster_vars = ['score','comms_num']
con_df1 = con_df1.dropna(subset=cluster_vars+['title'])

samp_df = con_df1
titles = {samp_df.title.loc[i] : i for i in samp_df.index.values}

scaler = StandardScaler()
scaler.fit(samp_df[cluster_vars].astype(float))
X = pd.DataFrame(scaler.transform(samp_df[cluster_vars].astype(float)), columns=cluster_vars)

kmeans = KMeans(n_clusters=5, 
                random_state=random_state)
kmeans.fit(X)
y_kmeans = kmeans.predict(X)
samp_df['cluster'] = (y_kmeans+1).astype(str)

vectorizer = CountVectorizer(
                             max_features=1000,
                             stop_words='english'
                            )

X_text = pd.DataFrame((vectorizer.fit_transform(con_df1.title_processed)>0).toarray())
all_words = vectorizer.get_feature_names()
d = {i : all_words[i] for i in range(len(all_words))}

tokens = [[d[j] for j in X_text.columns[X_text.loc[i]].tolist()] for i in range(X_text.shape[0])]

print(np.random.choice(tokens, 2))

dictionary = corpora.Dictionary(tokens)
corpus = [dictionary.doc2bow(text) for text in tokens]

ldamodel = LdaModel(corpus, num_topics=10, id2word=dictionary, passes=30, random_state=random_state)
topics = ldamodel.print_topics(num_words=6)
for topic in topics:
    print(topic)

lda_display = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary, sort_topics=False)
pyLDAvis.display(lda_display)